<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>big data on Median Watch</title>
    <link>https://medianwatch.netlify.com/tags/big-data/</link>
    <description>Recent content in big data on Median Watch</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <lastBuildDate>Mon, 03 Aug 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://medianwatch.netlify.com/tags/big-data/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Not waving but drowning in data</title>
      <link>https://medianwatch.netlify.com/post/not-waving/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.com/post/not-waving/</guid>
      <description>Our paper examining trends in acronyms in abstracts was recently published in eLife.We examined over 26 million abstracts from the PubMed database, which is easily the largest data set I’ve ever used.In this post I talk about some of the challenges and benefits of dealing with such a massive data set.
Data greedOne of the most common mistakes I see researchers — new and experienced — make is to collect too much data.</description>
    </item>
    
  </channel>
</rss>
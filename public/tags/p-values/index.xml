<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>p-values on Median Watch</title>
    <link>https://medianwatch.netlify.app/tags/p-values/</link>
    <description>Recent content in p-values on Median Watch</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <lastBuildDate>Wed, 23 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://medianwatch.netlify.app/tags/p-values/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Testing baseline tables in trials for signs of fraud</title>
      <link>https://medianwatch.netlify.app/post/baseline_testing/</link>
      <pubDate>Wed, 23 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.app/post/baseline_testing/</guid>
      <description>When fraudsters make up research data, they can make mistakes. Real data is rich and complex whilst fraudsters are on a get-rich-quick scheme and make slapdash errors.
One mistake they make is in randomised trials, where it&amp;rsquo;s standard to have a baseline table that compares the randomised groups. As the groups are randomised, the summary statistics should be similar. Fraudsters have no sense of &amp;lsquo;similar&amp;rsquo; and so have created data where the groups are nearly identical.</description>
    </item>
    
    <item>
      <title>Publishing &#34;negative&#34; results</title>
      <link>https://medianwatch.netlify.app/post/negative_results/</link>
      <pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.app/post/negative_results/</guid>
      <description>We&amp;rsquo;ve just published the world&amp;rsquo;s first randomised trial of funding (paper available here). We ran a truly novel study, using a gold standard study design, and with a published protocol. So why did it take two years to get through peer review?
Too much care Our team also recently published a large randomised trial about reducing unnecessary care at the end of life (paper available here).
A completely different area to funding, but again an important research question, with a strong study design, and a peer reviewed protocol.</description>
    </item>
    
    <item>
      <title>Statically significant</title>
      <link>https://medianwatch.netlify.app/post/statically_significant/</link>
      <pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.app/post/statically_significant/</guid>
      <description>A colleague sent me a draft manuscript with the typo &amp;ldquo;statically significant&amp;rdquo;. A typo that passes a spell check but would surely not pass reviewers and editors?
Oh dear, a PubMed search reveals that it has snuck past reviewers and editors, many many times. There are 975 abstracts that have used this nonsense phrase. There should be a celebration for the 1000th paper!
{width=80%,height=80%}
Surely that&amp;rsquo;s only in the terrible journals though?</description>
    </item>
    
    <item>
      <title>Publication bias or research misconduct?</title>
      <link>https://medianwatch.netlify.app/post/z_values/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.app/post/z_values/</guid>
      <description>In my talk on bad statistics in medical research, I showed the infamous plot of Z-values created by Erik van Zwet.
A version of the plot made with David Borg is shown below. The sample size is over 1.1 million Z-values.
{width=450px}
The two large spikes in Z-values are just below and above the statistically significant threshold of ± 1.96, corresponding to a p-value of less than 0.05. The plot looks like a Normal distribution that&amp;rsquo;s caved in.</description>
    </item>
    
    <item>
      <title>Celebrate hard science</title>
      <link>https://medianwatch.netlify.app/post/science_is_hard/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.app/post/science_is_hard/</guid>
      <description>Two weeks ago I gave a fun online talk on statistics for the Young Scientist Forum of the German Society for Biomaterials. I had some great chats with the organisers and there were good questions from the audience.
One good question was about how to interpret the analysis results when things are not clear cut. During my presentation I had talked about not deleting difficult outliers and not relying on p-values to give a falsely certain interpretation of what their results mean.</description>
    </item>
    
    <item>
      <title>A year without p-values</title>
      <link>https://medianwatch.netlify.app/post/pvalue_year/</link>
      <pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.app/post/pvalue_year/</guid>
      <description>One year ago after another stupid fight with a journal about p-values, I made a pledge to go without them for a year. Here’s how it went.
But first, why? I am aware of the arguments for and against p-values. I have used p-values for a long while and they can be a useful statistic.
The reason I ditched them is because almost nobody in health and medical research interprets them correctly, wrongly thinking they reveal the probability that the null hypothesis is true (other misinterpretations are available).</description>
    </item>
    
    <item>
      <title>Dear p-values, it&#39;s not me, it&#39;s not you, it&#39;s everyone else</title>
      <link>https://medianwatch.netlify.app/post/pvalue_pledge/</link>
      <pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://medianwatch.netlify.app/post/pvalue_pledge/</guid>
      <description>Yet another p-value run-in. For a recent observational study I tried to limit the use of p-values in the paper. My colleagues wanted more p-values and I had to politely push back. During one team meeting I even offered to put the p-values in if someone could accurately tell me what they meant … silence.
Predicting that the reviewers would also want to see more p-values, I added this sentence to the paper’s methods: “We have tried to limit the use of p-values, as they are often misunderstood or misinterpreted, and elected to discuss clinically meaningful differences.</description>
    </item>
    
  </channel>
</rss>

---
title: "Scientific fraud is rising, and automated systems won’t stop it. We need research detectives"
author: "Adrian Barnett"
date: "2023-06-23"
draft: false
tags:
  - "funding"
  - "fraud"
slug: research_fraud
showtoc: no
image: /img/rail_4.jpg
---



<p><em>Reposted from <a href="https://theconversation.com/scientific-fraud-is-rising-and-automated-systems-wont-stop-it-we-need-research-detectives-206235">The Conversation</a>.</em></p>
<p>Fraud in science is alarmingly common. Sometimes researchers <a href="https://retractionwatch.com/2016/03/31/neuroscientist-pleads-guilty-to-fraud-gets-two-year-suspended-sentence/">lie about results and invent data</a> to win funding and prestige. Other times, researchers might pay to stage and publish entirely bogus studies to win an undeserved pay rise – fuelling a “paper mill” industry worth <a href="https://ioppublishing.org/news/increasing-confidence-and-trust-in-research/">an estimated €1 billion a year</a>.</p>
<p>Some of this rubbish can be easily spotted by peer reviewers, but the peer review system has become badly stretched by ever-rising paper numbers. And there’s a new threat, as more sophisticated AI is able to <a href="https://www.nature.com/articles/d41586-023-01780-w">generate plausible scientific data</a>.</p>
<p>The latest idea among academic publishers is to use automated tools to screen all papers submitted to scientific journals for telltale signs. However, some of these tools are easy to fool.</p>
<p>I am part of a group of multidisciplinary scientists working to tackle research fraud and poor practice using <a href="https://en.wikipedia.org/wiki/Metascience">metascience</a> or the “science of science”. Ours is a new field, but we already have our own <a href="https://aimos.community/">society</a> and our members have worked with funders and publishers to investigate improvements to research practice.</p>
<div id="the-limits-of-automated-screening" class="section level2">
<h2>The limits of automated screening</h2>
<p>The problems with automated screening are highlighted by a <a href="https://www.science.org/content/article/fake-scientific-papers-are-alarmingly-common">new screening tool</a> publicised last month. The tool suggested around one in three neuroscience papers might be fraudulent.</p>
<p>However, this tool detects suspected fraud simply by flagging authors with a non-institutional email (such as gmail.com) and with a hospital affiliation. While this could catch some fraud, it will also flag many honest researchers, and the tool flagged a whopping 44% of genuine papers as potentially fake.</p>
<p>One big problem with simple screening tools is that fraudsters will quickly find workarounds. For instance, telling their clients to use their institutional email address to submit the paper.</p>
<p>Given the amount of money to be made, fraudsters have the time and motivation to find workarounds to automated screening systems.</p>
<p>This is not to say automated tools have no place. They have been used successfully to <a href="https://retractionwatch.com/2017/01/19/turned-cancer-researcher-literature-watchdog/">check papers for faulty experiments</a>, and to hunt for pilfered text reworked to <a href="https://retractionwatch.com/2021/07/19/tortured-phrases-lost-in-translation-sleuths-find-even-more-problems-at-journal-that-just-flagged-400-papers/">avoid plagiarism checkers</a>.</p>
<p>A <a href="https://www.stm-assoc.org/stm-integrity-hub/">project</a> launched by the International Association of Scientific, Technical and Medical Publishers which aims to use screening tools to tackle fraud is also welcome. But automated tools cannot be the only line of defence.</p>
</div>
<div id="a-crowdfunded-detective" class="section level2">
<h2>A crowdfunded detective</h2>
<p>There are remarkably few people who hunt through published research to detect scientific fraud. Perhaps the best known is the Dutch microbiologist Elisabeth Bik, who is an expert at catching manipulated images in scientific papers.</p>
<p>Bik has single-handedly caught multiple massive fraudsters, with the dodgy papers eventually being retracted from the scientific record.</p>
<p>Bik’s work is a tremendous public service. However, she isn’t paid by a university or a scientific publisher. Her detective work – which has seen her face <a href="https://www.theguardian.com/science/2021/may/22/world-expert-in-scientific-misconduct-faces-legal-action-for-challenging-integrity-of-hydroxychloroquine-study">harassment and court cases</a> – is <a href="https://www.bbc.co.uk/programmes/m001lqvg">crowd funded</a>.</p>
<p>With the billions of dollars in the publishing world, can’t a few million be found for quality control? In the meantime, one of our best-known lines of defence relies on good will and passion.</p>
<p>In Australia, spending just 0.1% of the annual scientific research budget on quality control would be A$12 million per year. This would be enough to fund a whole office of detectives and also training for researchers in good scientific practice, increasing the return on investment for the remaining 99.9% of the annual budget.</p>
</div>
<div id="call-the-fraud-police" class="section level2">
<h2>Call the fraud police</h2>
<p>A solution – or at least a partial one – seems obvious: somebody should employ lots of people like Bik to check quality. However, “somebody should” <a href="https://twitter.com/jamesheathers/status/1485980092367052804">is a dangerous phrase</a>, because it could easily mean nobody will.</p>
<p>Research funders wait for scientific publishers to take action. Publishers expect universities and other institutions to do something. Those institutions in turn look to government for a solution.</p>
<p>Meanwhile, paper mills are happily making a mint, and the world’s pool of scientific evidence is becoming increasingly contaminated by rubbish.</p>
<p>Quality control systems need not be expensive, as we don’t need to check every paper in detail. Random spot checks might be effective.</p>
<p>Say one in every 300 submissions gets checked by the “fraud police”. That’s a small probability, but people are notoriously bad at judging small probabilities, as proved by the popularity of lotteries.</p>
<p>There would also need to be consequences, such as notifying all the institutions and funders involved, and an expectation of a rapid response. If an institution were involved in multiple cases, publishers could flag all papers from that institution for extra checks.</p>
</div>
<div id="publicity-would-be-a-good-start" class="section level2">
<h2>Publicity would be a good start</h2>
<p>Of course, this could disadvantage honest researchers from that institution – but personally I would like to know if my colleagues had been submitting fraud. And given institutions rarely publicise the wrongdoing of their own staff, it may be the first I hear about it.</p>
<p>If honest researchers pressure their institutions to act, it would be a tremendous change. Publishers can’t be the only line of defence in tackling fraud.</p>
<p>Funding for <a href="https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-022-06080-6">stronger screening systems</a> is a great start, but we also need to spend money on people. We need to turn the arms race with the fraudsters into a brains race, because we have the better brains.</p>
<!-- Below is The Conversation's page counter tag. Please DO NOT REMOVE. -->
<p><img src="https://counter.theconversation.com/content/206235/count.gif?distributor=republish-lightbox-basic" alt="The Conversation" width="1" height="1" style="border: none !important; box-shadow: none !important; margin: 0 !important; max-height: 1px !important; max-width: 1px !important; min-height: 1px !important; min-width: 1px !important; opacity: 0 !important; outline: none !important; padding: 0 !important" referrerpolicy="no-referrer-when-downgrade" />
<!-- End of code. If you don't see any code above, please get new code from the Advanced tab after you click the republish button. The page counter does not collect any personal data. More info: https://theconversation.com/republishing-guidelines --></p>
</div>
